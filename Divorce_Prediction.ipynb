{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Divorce Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfEjEvGwFORq+9iWyslbPG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaimSonu/Dataset/blob/master/Divorce_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js8mdgl3Ia2-",
        "colab_type": "text"
      },
      "source": [
        "## Divorce Prediction using Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhrBfx3sIvDX",
        "colab_type": "text"
      },
      "source": [
        "**UCI Machine Learning Repository** has this dataset which contains some attributes which relate to divorce. Each person gives the attribute a score of 0,1,2,3 or 4 based on how he feels.\n",
        "Target is shown as 1 for divorce and 0 for no divorce\n",
        "This data can be used to predict the likeliness of divorce.\n",
        "\n",
        "**Attribute Information:**\n",
        "\n",
        "1. If one of us apologizes when our discussion deteriorates, the discussion ends.\n",
        "2. I know we can ignore our differences, even if things get hard sometimes.\n",
        "3. When we need it, we can take our discussions with my spouse from the beginning and correct it.\n",
        "4. When I discuss with my spouse, to contact him will eventually work.\n",
        "5. The time I spent with my wife is special for us.\n",
        "6. We don't have time at home as partners.\n",
        "7. We are like two strangers who share the same environment at home rather than family.\n",
        "8. I enjoy our holidays with my wife.\n",
        "9. I enjoy traveling with my wife.\n",
        "10. Most of our goals are common to my spouse.\n",
        "11. I think that one day in the future, when I look back, I see that my spouse and I have been in harmony with each other.\n",
        "12. My spouse and I have similar values in terms of personal freedom.\n",
        "13. My spouse and I have similar sense of entertainment.\n",
        "14. Most of our goals for people (children, friends, etc.) are the same.\n",
        "15. Our dreams with my spouse are similar and harmonious.\n",
        "16. We're compatible with my spouse about what love should be.\n",
        "17. We share the same views about being happy in our life with my spouse\n",
        "18. My spouse and I have similar ideas about how marriage should be\n",
        "19. My spouse and I have similar ideas about how roles should be in marriage\n",
        "20. My spouse and I have similar values in trust.\n",
        "21. I know exactly what my wife likes.\n",
        "22. I know how my spouse wants to be taken care of when she/he sick.\n",
        "23. I know my spouse's favorite food.\n",
        "24. I can tell you what kind of stress my spouse is facing in her/his life.\n",
        "25. I have knowledge of my spouse's inner world.\n",
        "26. I know my spouse's basic anxieties.\n",
        "27. I know what my spouse's current sources of stress are.\n",
        "28. I know my spouse's hopes and wishes.\n",
        "29. I know my spouse very well.\n",
        "30. I know my spouse's friends and their social relationships.\n",
        "31. I feel aggressive when I argue with my spouse.\n",
        "32. When discussing with my spouse, I usually use expressions such as ‘you always’ or ‘you never’ .\n",
        "33. I can use negative statements about my spouse's personality during our discussions.\n",
        "34. I can use offensive expressions during our discussions.\n",
        "35. I can insult my spouse during our discussions.\n",
        "36. I can be humiliating when we discussions.\n",
        "37. My discussion with my spouse is not calm.\n",
        "38. I hate my spouse's way of open a subject.\n",
        "39. Our discussions often occur suddenly.\n",
        "40. We're just starting a discussion before I know what's going on.\n",
        "41. When I talk to my spouse about something, my calm suddenly breaks.\n",
        "42. When I argue with my spouse, ı only go out and I don't say a word.\n",
        "43. I mostly stay silent to calm the environment a little bit.\n",
        "44. Sometimes I think it's good for me to leave home for a while.\n",
        "45. I'd rather stay silent than discuss with my spouse.\n",
        "46. Even if I'm right in the discussion, I stay silent to hurt my spouse.\n",
        "47. When I discuss with my spouse, I stay silent because I am afraid of not being able to control my anger.\n",
        "48. I feel right in our discussions.\n",
        "49. I have nothing to do with what I've been accused of.\n",
        "50. I'm not actually the one who's guilty about what I'm accused of.\n",
        "51. I'm not the one who's wrong about problems at home.\n",
        "52. I wouldn't hesitate to tell my spouse about her/his inadequacy.\n",
        "53. When I discuss, I remind my spouse of her/his inadequacy.\n",
        "54. I'm not afraid to tell my spouse about her/his incompetence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsBoghMaIwPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pA9tgy8I0Ds",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "8a85c163-95cf-46d4-f8df-96e392b23465"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ace7f4ed-308b-4ad9-8df1-86710c310c34\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ace7f4ed-308b-4ad9-8df1-86710c310c34\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving divorce.csv to divorce (7).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGfNO8URJCV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['divorce.csv']),sep=\";\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVlHW0s8LTPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b2c4d338-3b7e-4dfe-c73c-0eee286793e1"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Atr1</th>\n",
              "      <th>Atr2</th>\n",
              "      <th>Atr3</th>\n",
              "      <th>Atr4</th>\n",
              "      <th>Atr5</th>\n",
              "      <th>Atr6</th>\n",
              "      <th>Atr7</th>\n",
              "      <th>Atr8</th>\n",
              "      <th>Atr9</th>\n",
              "      <th>Atr10</th>\n",
              "      <th>Atr11</th>\n",
              "      <th>Atr12</th>\n",
              "      <th>Atr13</th>\n",
              "      <th>Atr14</th>\n",
              "      <th>Atr15</th>\n",
              "      <th>Atr16</th>\n",
              "      <th>Atr17</th>\n",
              "      <th>Atr18</th>\n",
              "      <th>Atr19</th>\n",
              "      <th>Atr20</th>\n",
              "      <th>Atr21</th>\n",
              "      <th>Atr22</th>\n",
              "      <th>Atr23</th>\n",
              "      <th>Atr24</th>\n",
              "      <th>Atr25</th>\n",
              "      <th>Atr26</th>\n",
              "      <th>Atr27</th>\n",
              "      <th>Atr28</th>\n",
              "      <th>Atr29</th>\n",
              "      <th>Atr30</th>\n",
              "      <th>Atr31</th>\n",
              "      <th>Atr32</th>\n",
              "      <th>Atr33</th>\n",
              "      <th>Atr34</th>\n",
              "      <th>Atr35</th>\n",
              "      <th>Atr36</th>\n",
              "      <th>Atr37</th>\n",
              "      <th>Atr38</th>\n",
              "      <th>Atr39</th>\n",
              "      <th>Atr40</th>\n",
              "      <th>Atr41</th>\n",
              "      <th>Atr42</th>\n",
              "      <th>Atr43</th>\n",
              "      <th>Atr44</th>\n",
              "      <th>Atr45</th>\n",
              "      <th>Atr46</th>\n",
              "      <th>Atr47</th>\n",
              "      <th>Atr48</th>\n",
              "      <th>Atr49</th>\n",
              "      <th>Atr50</th>\n",
              "      <th>Atr51</th>\n",
              "      <th>Atr52</th>\n",
              "      <th>Atr53</th>\n",
              "      <th>Atr54</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Atr1  Atr2  Atr3  Atr4  Atr5  ...  Atr51  Atr52  Atr53  Atr54  Target\n",
              "0     2     2     4     1     0  ...      2      3      2      1       1\n",
              "1     4     4     4     4     4  ...      4      4      2      2       1\n",
              "2     2     2     2     2     1  ...      1      2      2      2       1\n",
              "3     3     2     3     2     3  ...      3      2      2      2       1\n",
              "4     2     2     1     1     1  ...      2      2      1      0       1\n",
              "\n",
              "[5 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ786KYULUpF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "19d66900-40d0-4215-ceff-a5978a941905"
      },
      "source": [
        "df.describe()\n",
        "#we can infer that total data point is 170\n",
        "#from mean of each attribute we can get an overview of how people rated these attribute\n",
        "#Since mean of Target is 0.494118 we can conclude that the divorce rate is 49.4118%"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Atr1</th>\n",
              "      <th>Atr2</th>\n",
              "      <th>Atr3</th>\n",
              "      <th>Atr4</th>\n",
              "      <th>Atr5</th>\n",
              "      <th>Atr6</th>\n",
              "      <th>Atr7</th>\n",
              "      <th>Atr8</th>\n",
              "      <th>Atr9</th>\n",
              "      <th>Atr10</th>\n",
              "      <th>Atr11</th>\n",
              "      <th>Atr12</th>\n",
              "      <th>Atr13</th>\n",
              "      <th>Atr14</th>\n",
              "      <th>Atr15</th>\n",
              "      <th>Atr16</th>\n",
              "      <th>Atr17</th>\n",
              "      <th>Atr18</th>\n",
              "      <th>Atr19</th>\n",
              "      <th>Atr20</th>\n",
              "      <th>Atr21</th>\n",
              "      <th>Atr22</th>\n",
              "      <th>Atr23</th>\n",
              "      <th>Atr24</th>\n",
              "      <th>Atr25</th>\n",
              "      <th>Atr26</th>\n",
              "      <th>Atr27</th>\n",
              "      <th>Atr28</th>\n",
              "      <th>Atr29</th>\n",
              "      <th>Atr30</th>\n",
              "      <th>Atr31</th>\n",
              "      <th>Atr32</th>\n",
              "      <th>Atr33</th>\n",
              "      <th>Atr34</th>\n",
              "      <th>Atr35</th>\n",
              "      <th>Atr36</th>\n",
              "      <th>Atr37</th>\n",
              "      <th>Atr38</th>\n",
              "      <th>Atr39</th>\n",
              "      <th>Atr40</th>\n",
              "      <th>Atr41</th>\n",
              "      <th>Atr42</th>\n",
              "      <th>Atr43</th>\n",
              "      <th>Atr44</th>\n",
              "      <th>Atr45</th>\n",
              "      <th>Atr46</th>\n",
              "      <th>Atr47</th>\n",
              "      <th>Atr48</th>\n",
              "      <th>Atr49</th>\n",
              "      <th>Atr50</th>\n",
              "      <th>Atr51</th>\n",
              "      <th>Atr52</th>\n",
              "      <th>Atr53</th>\n",
              "      <th>Atr54</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>170.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.776471</td>\n",
              "      <td>1.652941</td>\n",
              "      <td>1.764706</td>\n",
              "      <td>1.482353</td>\n",
              "      <td>1.541176</td>\n",
              "      <td>0.747059</td>\n",
              "      <td>0.494118</td>\n",
              "      <td>1.452941</td>\n",
              "      <td>1.458824</td>\n",
              "      <td>1.576471</td>\n",
              "      <td>1.688235</td>\n",
              "      <td>1.652941</td>\n",
              "      <td>1.835294</td>\n",
              "      <td>1.570588</td>\n",
              "      <td>1.570588</td>\n",
              "      <td>1.476471</td>\n",
              "      <td>1.652941</td>\n",
              "      <td>1.517647</td>\n",
              "      <td>1.641176</td>\n",
              "      <td>1.458824</td>\n",
              "      <td>1.388235</td>\n",
              "      <td>1.247059</td>\n",
              "      <td>1.411765</td>\n",
              "      <td>1.511765</td>\n",
              "      <td>1.629412</td>\n",
              "      <td>1.488235</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>1.305882</td>\n",
              "      <td>1.494118</td>\n",
              "      <td>1.494118</td>\n",
              "      <td>2.123529</td>\n",
              "      <td>2.058824</td>\n",
              "      <td>1.805882</td>\n",
              "      <td>1.900000</td>\n",
              "      <td>1.670588</td>\n",
              "      <td>1.605882</td>\n",
              "      <td>2.088235</td>\n",
              "      <td>1.858824</td>\n",
              "      <td>2.088235</td>\n",
              "      <td>1.870588</td>\n",
              "      <td>1.994118</td>\n",
              "      <td>2.158824</td>\n",
              "      <td>2.705882</td>\n",
              "      <td>1.941176</td>\n",
              "      <td>2.458824</td>\n",
              "      <td>2.552941</td>\n",
              "      <td>2.270588</td>\n",
              "      <td>2.741176</td>\n",
              "      <td>2.382353</td>\n",
              "      <td>2.429412</td>\n",
              "      <td>2.476471</td>\n",
              "      <td>2.517647</td>\n",
              "      <td>2.241176</td>\n",
              "      <td>2.011765</td>\n",
              "      <td>0.494118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.627257</td>\n",
              "      <td>1.468654</td>\n",
              "      <td>1.415444</td>\n",
              "      <td>1.504327</td>\n",
              "      <td>1.632169</td>\n",
              "      <td>0.904046</td>\n",
              "      <td>0.898698</td>\n",
              "      <td>1.546371</td>\n",
              "      <td>1.557976</td>\n",
              "      <td>1.421529</td>\n",
              "      <td>1.647082</td>\n",
              "      <td>1.468654</td>\n",
              "      <td>1.478421</td>\n",
              "      <td>1.502765</td>\n",
              "      <td>1.506697</td>\n",
              "      <td>1.504246</td>\n",
              "      <td>1.614512</td>\n",
              "      <td>1.565998</td>\n",
              "      <td>1.641027</td>\n",
              "      <td>1.554173</td>\n",
              "      <td>1.452149</td>\n",
              "      <td>1.446529</td>\n",
              "      <td>1.612041</td>\n",
              "      <td>1.504385</td>\n",
              "      <td>1.530079</td>\n",
              "      <td>1.500447</td>\n",
              "      <td>1.457078</td>\n",
              "      <td>1.467788</td>\n",
              "      <td>1.592315</td>\n",
              "      <td>1.504420</td>\n",
              "      <td>1.646955</td>\n",
              "      <td>1.623445</td>\n",
              "      <td>1.785202</td>\n",
              "      <td>1.630515</td>\n",
              "      <td>1.842228</td>\n",
              "      <td>1.798412</td>\n",
              "      <td>1.716051</td>\n",
              "      <td>1.734802</td>\n",
              "      <td>1.719496</td>\n",
              "      <td>1.796039</td>\n",
              "      <td>1.721761</td>\n",
              "      <td>1.574034</td>\n",
              "      <td>1.348447</td>\n",
              "      <td>1.684267</td>\n",
              "      <td>1.499925</td>\n",
              "      <td>1.371786</td>\n",
              "      <td>1.586841</td>\n",
              "      <td>1.137348</td>\n",
              "      <td>1.511587</td>\n",
              "      <td>1.405090</td>\n",
              "      <td>1.260238</td>\n",
              "      <td>1.476537</td>\n",
              "      <td>1.505634</td>\n",
              "      <td>1.667611</td>\n",
              "      <td>0.501442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Atr1        Atr2        Atr3  ...       Atr53       Atr54      Target\n",
              "count  170.000000  170.000000  170.000000  ...  170.000000  170.000000  170.000000\n",
              "mean     1.776471    1.652941    1.764706  ...    2.241176    2.011765    0.494118\n",
              "std      1.627257    1.468654    1.415444  ...    1.505634    1.667611    0.501442\n",
              "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
              "25%      0.000000    0.000000    0.000000  ...    1.000000    0.000000    0.000000\n",
              "50%      2.000000    2.000000    2.000000  ...    2.000000    2.000000    0.000000\n",
              "75%      3.000000    3.000000    3.000000  ...    4.000000    4.000000    1.000000\n",
              "max      4.000000    4.000000    4.000000  ...    4.000000    4.000000    1.000000\n",
              "\n",
              "[8 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkuVTM-DLqQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "01e25d50-8a1e-41db-9d90-d6b59cf6aa5a"
      },
      "source": [
        "df.corr()[\"Target\"]\n",
        "#We can check the relation of each attribute with Target(divorce)."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Atr1      0.861324\n",
              "Atr2      0.820774\n",
              "Atr3      0.806709\n",
              "Atr4      0.819583\n",
              "Atr5      0.893180\n",
              "Atr6      0.420913\n",
              "Atr7      0.544835\n",
              "Atr8      0.869569\n",
              "Atr9      0.912368\n",
              "Atr10     0.834897\n",
              "Atr11     0.918386\n",
              "Atr12     0.868983\n",
              "Atr13     0.844743\n",
              "Atr14     0.864316\n",
              "Atr15     0.901220\n",
              "Atr16     0.886260\n",
              "Atr17     0.929346\n",
              "Atr18     0.923208\n",
              "Atr19     0.928627\n",
              "Atr20     0.907008\n",
              "Atr21     0.864519\n",
              "Atr22     0.825938\n",
              "Atr23     0.837504\n",
              "Atr24     0.839392\n",
              "Atr25     0.857052\n",
              "Atr26     0.872868\n",
              "Atr27     0.869788\n",
              "Atr28     0.846606\n",
              "Atr29     0.892954\n",
              "Atr30     0.874531\n",
              "Atr31     0.792607\n",
              "Atr32     0.829056\n",
              "Atr33     0.861328\n",
              "Atr34     0.835167\n",
              "Atr35     0.862624\n",
              "Atr36     0.886497\n",
              "Atr37     0.863597\n",
              "Atr38     0.883311\n",
              "Atr39     0.896180\n",
              "Atr40     0.938684\n",
              "Atr41     0.894356\n",
              "Atr42     0.739629\n",
              "Atr43     0.566242\n",
              "Atr44     0.847336\n",
              "Atr45     0.546450\n",
              "Atr46     0.443465\n",
              "Atr47     0.656409\n",
              "Atr48     0.619830\n",
              "Atr49     0.740704\n",
              "Atr50     0.755248\n",
              "Atr51     0.692681\n",
              "Atr52     0.651478\n",
              "Atr53     0.711176\n",
              "Atr54     0.806765\n",
              "Target    1.000000\n",
              "Name: Target, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsvRLtWPLtFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "07b039c3-e86b-483c-c9a8-64ffca597609"
      },
      "source": [
        "plt.figure(figsize=(8,4),dpi=120)\n",
        "sns.scatterplot(range(0,55),df.corr()[\"Target\"],color=\"red\")\n",
        "plt.xlabel(\"Attribute number\")\n",
        "#Since df.corr()[\"Target\"] is harder to comprehend we can plot a scatterplot to get an idea.\n",
        "#We can see how much each factor affect Target, higher value show more chance of divorce."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Attribute number')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz4AAAG4CAYAAACEvTI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 960x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzMuXgmjLvOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "efc0254a-427b-4765-85bc-f71348473a39"
      },
      "source": [
        "sns.countplot(df[\"Target\"])\n",
        "#we can see that both are nearly equal, that's what we obsered from 49.4118% divorce rate"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7973976470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_AHAy6_Lxsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.drop([\"Target\"],axis=1)\n",
        "y = df[\"Target\"]\n",
        "#seperate Target from factors affecting it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Gs8qOdLze3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#import the standard scaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHh3-8ulL06m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scaler = StandardScaler()\n",
        "#Instantiate the scaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gmJ35HCL2Tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scaler.fit(X)\n",
        "#Fit data on the scaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkd3GVHaL3pO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X=scaler.transform(X)\n",
        "#scale the X data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPBv08nNL482",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#if the data is scaled then the input passed into the predictor should also be scaled first."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxpgFvrtL6Eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJKFmtsbL7PF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.35,random_state=51)\n",
        "#Split the data as testing and training data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGYJWGdAPgfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_s=scaler.transform(X_train)\n",
        "scaler.fit(X_test)\n",
        "X_test_s=scaler.fit(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA-flsipL8ZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "#Grid Search is used to find best parameters for a model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3BMidzQL_ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "#Used for getting scores and confusion matrix of model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FccrK_6fMCOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "#Get the accuracy of the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2yXhhOVMFew",
        "colab_type": "text"
      },
      "source": [
        "**Principal Component Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eNQmNl_MF59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3uQlN6rMIw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKl7C7dyMJ4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca.fit(X_train)\n",
        "X_train_p = pca.transform(X_train)\n",
        "pca.fit(X_test)\n",
        "X_test_p = pca.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GGHT1qDMM6Z",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvNa8r9XMNrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "#import the Logistic Regression Classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOzI4HwLMRIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrnHxR_uMXkG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b15a2914-03b9-4fc8-bcaf-640d841215ee"
      },
      "source": [
        "log.fit(X_train,y_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW5vLRe6MY9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_log = log.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ-m2PYwMau2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "9edc39a5-cdb5-4d72-c9c6-dea26ceb2862"
      },
      "source": [
        "print(classification_report(y_test,pred_log))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98        30\n",
            "           1       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.98        60\n",
            "   macro avg       0.98      0.98      0.98        60\n",
            "weighted avg       0.98      0.98      0.98        60\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBmZBU4cMbt2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f4f96ff6-c67a-4b12-c53a-f03dd3fbbbb4"
      },
      "source": [
        "confusion_matrix(y_test,pred_log)\n",
        "#This is actually a pretty good result with only 2 misclassifications."
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30,  0],\n",
              "       [ 1, 29]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhwQYQUpMdWO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a49f9b93-610c-46a8-f387-17cd1925e4d3"
      },
      "source": [
        "accuracy_score(y_test,pred_log)*100\n",
        "#This is the accuracy percentage of logistic regression model"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.33333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnhXZjDTMeg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params_grid_log = {\"C\":[0.1,1,10,100],\"tol\":[1,0.1,0.01,0.001,0.0001]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90euEDoYMfwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_log = GridSearchCV(LogisticRegression(),params_grid_log,verbose=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHj7KYfIMhvu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3872382d-12e3-4a19-d8b9-e1889172a34f"
      },
      "source": [
        "grid_log.fit(X_train,y_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "[CV] C=0.1, tol=1 ....................................................\n",
            "[CV] ........................ C=0.1, tol=1, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=1 ....................................................\n",
            "[CV] ........................ C=0.1, tol=1, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=1 ....................................................\n",
            "[CV] ........................ C=0.1, tol=1, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=1 ....................................................\n",
            "[CV] ........................ C=0.1, tol=1, score=1.000, total=   0.0s\n",
            "[CV] C=0.1, tol=1 ....................................................\n",
            "[CV] ........................ C=0.1, tol=1, score=1.000, total=   0.0s\n",
            "[CV] C=0.1, tol=0.1 ..................................................\n",
            "[CV] ...................... C=0.1, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.1 ..................................................\n",
            "[CV] ...................... C=0.1, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.1 ..................................................\n",
            "[CV] ...................... C=0.1, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.1 ..................................................\n",
            "[CV] ...................... C=0.1, tol=0.1, score=1.000, total=   0.0s\n",
            "[CV] C=0.1, tol=0.1 ..................................................\n",
            "[CV] ...................... C=0.1, tol=0.1, score=1.000, total=   0.0s\n",
            "[CV] C=0.1, tol=0.01 .................................................\n",
            "[CV] ..................... C=0.1, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.01 .................................................\n",
            "[CV] ..................... C=0.1, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.01 .................................................\n",
            "[CV] ..................... C=0.1, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.01 .................................................\n",
            "[CV] ..................... C=0.1, tol=0.01, score=1.000, total=   0.0s\n",
            "[CV] C=0.1, tol=0.01 .................................................\n",
            "[CV] ..................... C=0.1, tol=0.01, score=1.000, total=   0.0s\n",
            "[CV] C=0.1, tol=0.001 ................................................\n",
            "[CV] .................... C=0.1, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.001 ................................................\n",
            "[CV] .................... C=0.1, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.001 ................................................\n",
            "[CV] .................... C=0.1, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.001 ................................................\n",
            "[CV] .................... C=0.1, tol=0.001, score=1.000, total=   0.0s\n",
            "[CV] C=0.1, tol=0.001 ................................................\n",
            "[CV] .................... C=0.1, tol=0.001, score=1.000, total=   0.0s\n",
            "[CV] C=0.1, tol=0.0001 ...............................................\n",
            "[CV] ................... C=0.1, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.0001 ...............................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ................... C=0.1, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.0001 ...............................................\n",
            "[CV] ................... C=0.1, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=0.1, tol=0.0001 ...............................................\n",
            "[CV] ................... C=0.1, tol=0.0001, score=1.000, total=   0.0s\n",
            "[CV] C=0.1, tol=0.0001 ...............................................\n",
            "[CV] ................... C=0.1, tol=0.0001, score=1.000, total=   0.0s\n",
            "[CV] C=1, tol=1 ......................................................\n",
            "[CV] .......................... C=1, tol=1, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=1 ......................................................\n",
            "[CV] .......................... C=1, tol=1, score=0.909, total=   0.0s\n",
            "[CV] C=1, tol=1 ......................................................\n",
            "[CV] .......................... C=1, tol=1, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=1 ......................................................\n",
            "[CV] .......................... C=1, tol=1, score=1.000, total=   0.0s\n",
            "[CV] C=1, tol=1 ......................................................\n",
            "[CV] .......................... C=1, tol=1, score=1.000, total=   0.0s\n",
            "[CV] C=1, tol=0.1 ....................................................\n",
            "[CV] ........................ C=1, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.1 ....................................................\n",
            "[CV] ........................ C=1, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.1 ....................................................\n",
            "[CV] ........................ C=1, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.1 ....................................................\n",
            "[CV] ........................ C=1, tol=0.1, score=1.000, total=   0.0s\n",
            "[CV] C=1, tol=0.1 ....................................................\n",
            "[CV] ........................ C=1, tol=0.1, score=1.000, total=   0.0s\n",
            "[CV] C=1, tol=0.01 ...................................................\n",
            "[CV] ....................... C=1, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.01 ...................................................\n",
            "[CV] ....................... C=1, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.01 ...................................................\n",
            "[CV] ....................... C=1, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.01 ...................................................\n",
            "[CV] ....................... C=1, tol=0.01, score=1.000, total=   0.0s\n",
            "[CV] C=1, tol=0.01 ...................................................\n",
            "[CV] ....................... C=1, tol=0.01, score=1.000, total=   0.0s\n",
            "[CV] C=1, tol=0.001 ..................................................\n",
            "[CV] ...................... C=1, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.001 ..................................................\n",
            "[CV] ...................... C=1, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.001 ..................................................\n",
            "[CV] ...................... C=1, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.001 ..................................................\n",
            "[CV] ...................... C=1, tol=0.001, score=1.000, total=   0.0s\n",
            "[CV] C=1, tol=0.001 ..................................................\n",
            "[CV] ...................... C=1, tol=0.001, score=1.000, total=   0.0s\n",
            "[CV] C=1, tol=0.0001 .................................................\n",
            "[CV] ..................... C=1, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.0001 .................................................\n",
            "[CV] ..................... C=1, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.0001 .................................................\n",
            "[CV] ..................... C=1, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=1, tol=0.0001 .................................................\n",
            "[CV] ..................... C=1, tol=0.0001, score=1.000, total=   0.0s\n",
            "[CV] C=1, tol=0.0001 .................................................\n",
            "[CV] ..................... C=1, tol=0.0001, score=1.000, total=   0.0s\n",
            "[CV] C=10, tol=1 .....................................................\n",
            "[CV] ......................... C=10, tol=1, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=1 .....................................................\n",
            "[CV] ......................... C=10, tol=1, score=0.909, total=   0.0s\n",
            "[CV] C=10, tol=1 .....................................................\n",
            "[CV] ......................... C=10, tol=1, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=1 .....................................................\n",
            "[CV] ......................... C=10, tol=1, score=1.000, total=   0.0s\n",
            "[CV] C=10, tol=1 .....................................................\n",
            "[CV] ......................... C=10, tol=1, score=1.000, total=   0.0s\n",
            "[CV] C=10, tol=0.1 ...................................................\n",
            "[CV] ....................... C=10, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.1 ...................................................\n",
            "[CV] ....................... C=10, tol=0.1, score=0.909, total=   0.0s\n",
            "[CV] C=10, tol=0.1 ...................................................\n",
            "[CV] ....................... C=10, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.1 ...................................................\n",
            "[CV] ....................... C=10, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.1 ...................................................\n",
            "[CV] ....................... C=10, tol=0.1, score=1.000, total=   0.0s\n",
            "[CV] C=10, tol=0.01 ..................................................\n",
            "[CV] ...................... C=10, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.01 ..................................................\n",
            "[CV] ...................... C=10, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.01 ..................................................\n",
            "[CV] ...................... C=10, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.01 ..................................................\n",
            "[CV] ...................... C=10, tol=0.01, score=1.000, total=   0.0s\n",
            "[CV] C=10, tol=0.01 ..................................................\n",
            "[CV] ...................... C=10, tol=0.01, score=1.000, total=   0.0s\n",
            "[CV] C=10, tol=0.001 .................................................\n",
            "[CV] ..................... C=10, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.001 .................................................\n",
            "[CV] ..................... C=10, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.001 .................................................\n",
            "[CV] ..................... C=10, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.001 .................................................\n",
            "[CV] ..................... C=10, tol=0.001, score=1.000, total=   0.0s\n",
            "[CV] C=10, tol=0.001 .................................................\n",
            "[CV] ..................... C=10, tol=0.001, score=1.000, total=   0.0s\n",
            "[CV] C=10, tol=0.0001 ................................................\n",
            "[CV] .................... C=10, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.0001 ................................................\n",
            "[CV] .................... C=10, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.0001 ................................................\n",
            "[CV] .................... C=10, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=10, tol=0.0001 ................................................\n",
            "[CV] .................... C=10, tol=0.0001, score=1.000, total=   0.0s\n",
            "[CV] C=10, tol=0.0001 ................................................\n",
            "[CV] .................... C=10, tol=0.0001, score=1.000, total=   0.0s\n",
            "[CV] C=100, tol=1 ....................................................\n",
            "[CV] ........................ C=100, tol=1, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=1 ....................................................\n",
            "[CV] ........................ C=100, tol=1, score=0.909, total=   0.0s\n",
            "[CV] C=100, tol=1 ....................................................\n",
            "[CV] ........................ C=100, tol=1, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=1 ....................................................\n",
            "[CV] ........................ C=100, tol=1, score=1.000, total=   0.0s\n",
            "[CV] C=100, tol=1 ....................................................\n",
            "[CV] ........................ C=100, tol=1, score=1.000, total=   0.0s\n",
            "[CV] C=100, tol=0.1 ..................................................\n",
            "[CV] ...................... C=100, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.1 ..................................................\n",
            "[CV] ...................... C=100, tol=0.1, score=0.909, total=   0.0s\n",
            "[CV] C=100, tol=0.1 ..................................................\n",
            "[CV] ...................... C=100, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.1 ..................................................\n",
            "[CV] ...................... C=100, tol=0.1, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.1 ..................................................\n",
            "[CV] ...................... C=100, tol=0.1, score=1.000, total=   0.0s\n",
            "[CV] C=100, tol=0.01 .................................................\n",
            "[CV] ..................... C=100, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.01 .................................................\n",
            "[CV] ..................... C=100, tol=0.01, score=0.909, total=   0.0s\n",
            "[CV] C=100, tol=0.01 .................................................\n",
            "[CV] ..................... C=100, tol=0.01, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.01 .................................................\n",
            "[CV] ..................... C=100, tol=0.01, score=1.000, total=   0.0s\n",
            "[CV] C=100, tol=0.01 .................................................\n",
            "[CV] ..................... C=100, tol=0.01, score=1.000, total=   0.0s\n",
            "[CV] C=100, tol=0.001 ................................................\n",
            "[CV] .................... C=100, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.001 ................................................\n",
            "[CV] .................... C=100, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.001 ................................................\n",
            "[CV] .................... C=100, tol=0.001, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.001 ................................................\n",
            "[CV] .................... C=100, tol=0.001, score=1.000, total=   0.0s\n",
            "[CV] C=100, tol=0.001 ................................................\n",
            "[CV] .................... C=100, tol=0.001, score=1.000, total=   0.0s\n",
            "[CV] C=100, tol=0.0001 ...............................................\n",
            "[CV] ................... C=100, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.0001 ...............................................\n",
            "[CV] ................... C=100, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.0001 ...............................................\n",
            "[CV] ................... C=100, tol=0.0001, score=0.955, total=   0.0s\n",
            "[CV] C=100, tol=0.0001 ...............................................\n",
            "[CV] ................... C=100, tol=0.0001, score=1.000, total=   0.0s\n",
            "[CV] C=100, tol=0.0001 ...............................................\n",
            "[CV] ................... C=100, tol=0.0001, score=1.000, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': [0.1, 1, 10, 100],\n",
              "                         'tol': [1, 0.1, 0.01, 0.001, 0.0001]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eij1r_f7Mi9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ab38c98-1059-477d-96b1-01cbfadf6bc4"
      },
      "source": [
        "grid_log.best_params_\n",
        "#best parameters for getting max accuracy"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.1, 'tol': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYlHof_gMlQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_log_grid = grid_log.predict(X_test)\n",
        "#predicting using best parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eotKcuwkMmp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8fdb05a-574a-4c58-ebd6-44618a311aab"
      },
      "source": [
        "accuracy_score(y_test,pred_log_grid)\n",
        "#No increase in accuracy, which isn't actually a bad thing.\n",
        "#Atleast the model didn't try to overfit the data."
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9833333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QvR1HyyMo-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d58b37f9-f647-4187-81eb-7fa609243c04"
      },
      "source": [
        "log.fit(X_train_p,y_train)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_xmHzhoMqlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_log_p=log.predict(X_test_p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQt63_2iMsJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb3a7d2b-8f15-4b52-fd2c-0ef10eeeaf34"
      },
      "source": [
        "accuracy_score(y_test,pred_log_p)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLNT3AY3Mtme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "5a6df0cd-a2a1-4866-bb83-af7b292ae411"
      },
      "source": [
        "print(classification_report(y_test,pred_log_p))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        30\n",
            "           1       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           1.00        60\n",
            "   macro avg       1.00      1.00      1.00        60\n",
            "weighted avg       1.00      1.00      1.00        60\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL6vBXtTMutu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d9705e52-cd6c-4bd6-92a4-2725298367a6"
      },
      "source": [
        "confusion_matrix(y_test,pred_log_p)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30,  0],\n",
              "       [ 0, 30]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jiPbw2QNSZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Great !!!!!! We acheived 100% accuracy\n",
        "#Here we can observe that accuracy has increased after using PCA\n",
        "#Its not always neccesary that PCA increases accuracy it actually reduces computing time."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8DyNYI1NpP7",
        "colab_type": "text"
      },
      "source": [
        "**K Nearest Neighbour Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edAxoLEbNp1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKHI-A-8NrvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn = KNeighborsClassifier()\n",
        "#for now let \"k\" be default"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj0HDZtbNtxG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ebf722f9-1d4c-49a6-9336-b188393d96e9"
      },
      "source": [
        "knn.fit(X_train,y_train)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlFcjybHNvA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_knn = knn.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqLca8ZVNwXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "791f1c51-37a5-4252-9bc0-783fb7f798ee"
      },
      "source": [
        "print(classification_report(y_test,pred_knn))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98        30\n",
            "           1       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.98        60\n",
            "   macro avg       0.98      0.98      0.98        60\n",
            "weighted avg       0.98      0.98      0.98        60\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehC4IEqHNx0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lets try to optimise this model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t46h0Z2BNzeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error_rate=[]\n",
        "for i in range(1,51):\n",
        "    knn_op=KNeighborsClassifier(n_neighbors=i)\n",
        "    knn_op.fit(X_train,y_train)\n",
        "    pred_i=knn_op.predict(X_test)\n",
        "    error_rate.append(np.mean(pred_i != y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwoONqx-N0yG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "c36737e7-fa30-4afc-cb0f-93b6109ab515"
      },
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(range(1,51),error_rate, color=\"blue\", linestyle=\"--\", marker=\"o\", markersize=10, markerfacecolor=\"red\")\n",
        "plt.title(\"Error rate vs K value\")\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"Error rate\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Error rate')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAEWCAYAAADo/9QWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97bPIwPaN2YG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a0faf1a-2113-4577-9920-92fc3fa78e6f"
      },
      "source": [
        "accuracy_score(y_test,pred_knn)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9833333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESWlHNVvN4N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from above we can infer that the accuracy isn't really changing with \"k\" value.\n",
        "#Most of the times, almost every time it changes and we pick the \"k\" with least error."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzs8YwLCN6G3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c0c85d5b-8b88-4bd1-9964-6cee22de3bdb"
      },
      "source": [
        "#Using PCA data\n",
        "knn.fit(X_train_p,y_train)\n",
        "pred_knn_p = knn.predict(X_test_p)\n",
        "print(classification_report(y_test,pred_knn_p))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98        30\n",
            "           1       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.98        60\n",
            "   macro avg       0.98      0.98      0.98        60\n",
            "weighted avg       0.98      0.98      0.98        60\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6XsEJ9rN8ZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEy1jJWxN-Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rtree = RandomForestClassifier(n_estimators=400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMWz8z1CN_RO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7330c2b4-0603-480d-911c-a9cffba2377e"
      },
      "source": [
        "rtree.fit(X_train,y_train)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-V2F3BsOAaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_rtee = rtree.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE4oPaCQOBvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2e9b964-6292-4210-f1d9-2d4c1afdc0bc"
      },
      "source": [
        "accuracy_score(y_test,pred_rtee)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9833333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1Tm0ceBOC7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trt2tcUGOE1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svc = SVC()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVIMbTlOOF-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "97984ef1-d781-42f6-c5fa-d26afbfbb7c6"
      },
      "source": [
        "svc.fit(X_train,y_train)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jy2a0KMOHAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_svc = svc.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3EhSv01OIFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc3b37b6-9699-4401-f00a-6926783f0255"
      },
      "source": [
        "accuracy_score(y_test,pred_svc)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9833333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNVRXIN5OM1U",
        "colab_type": "text"
      },
      "source": [
        "**Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASte4g7sOKxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkKfGn2DORr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb = GaussianNB()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcq51hFXOTWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7945ecfd-aea1-4adf-a826-5bc564ef37c6"
      },
      "source": [
        "nb.fit(X_train,y_train)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKhAuhrYOU0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_nb = nb.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IH7ImkDOV8m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "3b21c3b7-2bb5-44c6-8b14-15850077b714"
      },
      "source": [
        "print(classification_report(y_test,pred_nb))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98        30\n",
            "           1       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.98        60\n",
            "   macro avg       0.98      0.98      0.98        60\n",
            "weighted avg       0.98      0.98      0.98        60\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDWmWcSYOZHi",
        "colab_type": "text"
      },
      "source": [
        "**Artificial Neural Networks (ANN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OErWoYbnOZhO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "42a595e8-b3af-487c-cae7-79d7b819bead"
      },
      "source": [
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojZOVWEMOcPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cZGwxk-Op4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ann = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9yhYMLWOrPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ann.add(Dense(6,activation='relu'))\n",
        "ann.add(Dense(20,activation='relu'))\n",
        "ann.add(Dense(6,activation='relu'))\n",
        "#adding layers to the model\n",
        "\n",
        "ann.add(Dense(1))\n",
        "\n",
        "ann.compile(optimizer='adam',loss='mse', metrics=[\"accuracy\"])\n",
        "#minimizes loss \"msle\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPrmzefdOsae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "737a3a63-f1af-4ef9-b6e5-2c36f487ad53"
      },
      "source": [
        "ann.fit(x=X_train_s,y=y_train.values,epochs=250,verbose=1)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 110 samples\n",
            "Epoch 1/250\n",
            "110/110 [==============================] - 0s 817us/sample - loss: 0.6245 - acc: 0.5091\n",
            "Epoch 2/250\n",
            "110/110 [==============================] - 0s 133us/sample - loss: 0.5334 - acc: 0.5091\n",
            "Epoch 3/250\n",
            "110/110 [==============================] - 0s 93us/sample - loss: 0.4569 - acc: 0.5091\n",
            "Epoch 4/250\n",
            "110/110 [==============================] - 0s 82us/sample - loss: 0.3919 - acc: 0.5091\n",
            "Epoch 5/250\n",
            "110/110 [==============================] - 0s 106us/sample - loss: 0.3325 - acc: 0.5091\n",
            "Epoch 6/250\n",
            "110/110 [==============================] - 0s 89us/sample - loss: 0.2741 - acc: 0.5091\n",
            "Epoch 7/250\n",
            "110/110 [==============================] - 0s 96us/sample - loss: 0.2205 - acc: 0.5636\n",
            "Epoch 8/250\n",
            "110/110 [==============================] - 0s 112us/sample - loss: 0.1688 - acc: 0.7455\n",
            "Epoch 9/250\n",
            "110/110 [==============================] - 0s 110us/sample - loss: 0.1250 - acc: 0.8364\n",
            "Epoch 10/250\n",
            "110/110 [==============================] - 0s 113us/sample - loss: 0.0897 - acc: 0.9182\n",
            "Epoch 11/250\n",
            "110/110 [==============================] - 0s 99us/sample - loss: 0.0669 - acc: 0.9273\n",
            "Epoch 12/250\n",
            "110/110 [==============================] - 0s 85us/sample - loss: 0.0538 - acc: 0.9545\n",
            "Epoch 13/250\n",
            "110/110 [==============================] - 0s 88us/sample - loss: 0.0508 - acc: 0.9545\n",
            "Epoch 14/250\n",
            "110/110 [==============================] - 0s 92us/sample - loss: 0.0513 - acc: 0.9636\n",
            "Epoch 15/250\n",
            "110/110 [==============================] - 0s 103us/sample - loss: 0.0522 - acc: 0.9636\n",
            "Epoch 16/250\n",
            "110/110 [==============================] - 0s 100us/sample - loss: 0.0514 - acc: 0.9636\n",
            "Epoch 17/250\n",
            "110/110 [==============================] - 0s 119us/sample - loss: 0.0493 - acc: 0.9636\n",
            "Epoch 18/250\n",
            "110/110 [==============================] - 0s 123us/sample - loss: 0.0469 - acc: 0.9636\n",
            "Epoch 19/250\n",
            "110/110 [==============================] - 0s 123us/sample - loss: 0.0454 - acc: 0.9636\n",
            "Epoch 20/250\n",
            "110/110 [==============================] - 0s 117us/sample - loss: 0.0442 - acc: 0.9636\n",
            "Epoch 21/250\n",
            "110/110 [==============================] - 0s 105us/sample - loss: 0.0438 - acc: 0.9636\n",
            "Epoch 22/250\n",
            "110/110 [==============================] - 0s 101us/sample - loss: 0.0432 - acc: 0.9636\n",
            "Epoch 23/250\n",
            "110/110 [==============================] - 0s 110us/sample - loss: 0.0424 - acc: 0.9636\n",
            "Epoch 24/250\n",
            "110/110 [==============================] - 0s 101us/sample - loss: 0.0414 - acc: 0.9636\n",
            "Epoch 25/250\n",
            "110/110 [==============================] - 0s 88us/sample - loss: 0.0407 - acc: 0.9636\n",
            "Epoch 26/250\n",
            "110/110 [==============================] - 0s 108us/sample - loss: 0.0400 - acc: 0.9636\n",
            "Epoch 27/250\n",
            "110/110 [==============================] - 0s 96us/sample - loss: 0.0392 - acc: 0.9636\n",
            "Epoch 28/250\n",
            "110/110 [==============================] - 0s 105us/sample - loss: 0.0387 - acc: 0.9636\n",
            "Epoch 29/250\n",
            "110/110 [==============================] - 0s 115us/sample - loss: 0.0381 - acc: 0.9636\n",
            "Epoch 30/250\n",
            "110/110 [==============================] - 0s 99us/sample - loss: 0.0375 - acc: 0.9636\n",
            "Epoch 31/250\n",
            "110/110 [==============================] - 0s 114us/sample - loss: 0.0369 - acc: 0.9636\n",
            "Epoch 32/250\n",
            "110/110 [==============================] - 0s 120us/sample - loss: 0.0363 - acc: 0.9636\n",
            "Epoch 33/250\n",
            "110/110 [==============================] - 0s 98us/sample - loss: 0.0358 - acc: 0.9727\n",
            "Epoch 34/250\n",
            "110/110 [==============================] - 0s 82us/sample - loss: 0.0352 - acc: 0.9727\n",
            "Epoch 35/250\n",
            "110/110 [==============================] - 0s 109us/sample - loss: 0.0346 - acc: 0.9727\n",
            "Epoch 36/250\n",
            "110/110 [==============================] - 0s 117us/sample - loss: 0.0341 - acc: 0.9727\n",
            "Epoch 37/250\n",
            "110/110 [==============================] - 0s 124us/sample - loss: 0.0338 - acc: 0.9727\n",
            "Epoch 38/250\n",
            "110/110 [==============================] - 0s 144us/sample - loss: 0.0332 - acc: 0.9727\n",
            "Epoch 39/250\n",
            "110/110 [==============================] - 0s 100us/sample - loss: 0.0328 - acc: 0.9727\n",
            "Epoch 40/250\n",
            "110/110 [==============================] - 0s 113us/sample - loss: 0.0324 - acc: 0.9727\n",
            "Epoch 41/250\n",
            "110/110 [==============================] - 0s 195us/sample - loss: 0.0320 - acc: 0.9727\n",
            "Epoch 42/250\n",
            "110/110 [==============================] - 0s 84us/sample - loss: 0.0316 - acc: 0.9727\n",
            "Epoch 43/250\n",
            "110/110 [==============================] - 0s 108us/sample - loss: 0.0312 - acc: 0.9727\n",
            "Epoch 44/250\n",
            "110/110 [==============================] - 0s 111us/sample - loss: 0.0309 - acc: 0.9727\n",
            "Epoch 45/250\n",
            "110/110 [==============================] - 0s 111us/sample - loss: 0.0305 - acc: 0.9727\n",
            "Epoch 46/250\n",
            "110/110 [==============================] - 0s 106us/sample - loss: 0.0300 - acc: 0.9727\n",
            "Epoch 47/250\n",
            "110/110 [==============================] - 0s 113us/sample - loss: 0.0296 - acc: 0.9727\n",
            "Epoch 48/250\n",
            "110/110 [==============================] - 0s 75us/sample - loss: 0.0291 - acc: 0.9727\n",
            "Epoch 49/250\n",
            "110/110 [==============================] - 0s 85us/sample - loss: 0.0287 - acc: 0.9727\n",
            "Epoch 50/250\n",
            "110/110 [==============================] - 0s 87us/sample - loss: 0.0283 - acc: 0.9727\n",
            "Epoch 51/250\n",
            "110/110 [==============================] - 0s 100us/sample - loss: 0.0279 - acc: 0.9727\n",
            "Epoch 52/250\n",
            "110/110 [==============================] - 0s 85us/sample - loss: 0.0276 - acc: 0.9727\n",
            "Epoch 53/250\n",
            "110/110 [==============================] - 0s 118us/sample - loss: 0.0272 - acc: 0.9727\n",
            "Epoch 54/250\n",
            "110/110 [==============================] - 0s 113us/sample - loss: 0.0268 - acc: 0.9727\n",
            "Epoch 55/250\n",
            "110/110 [==============================] - 0s 128us/sample - loss: 0.0264 - acc: 0.9727\n",
            "Epoch 56/250\n",
            "110/110 [==============================] - 0s 110us/sample - loss: 0.0259 - acc: 0.9727\n",
            "Epoch 57/250\n",
            "110/110 [==============================] - 0s 97us/sample - loss: 0.0255 - acc: 0.9727\n",
            "Epoch 58/250\n",
            "110/110 [==============================] - 0s 98us/sample - loss: 0.0252 - acc: 0.9727\n",
            "Epoch 59/250\n",
            "110/110 [==============================] - 0s 92us/sample - loss: 0.0248 - acc: 0.9727\n",
            "Epoch 60/250\n",
            "110/110 [==============================] - 0s 91us/sample - loss: 0.0244 - acc: 0.9727\n",
            "Epoch 61/250\n",
            "110/110 [==============================] - 0s 96us/sample - loss: 0.0240 - acc: 0.9727\n",
            "Epoch 62/250\n",
            "110/110 [==============================] - 0s 94us/sample - loss: 0.0236 - acc: 0.9727\n",
            "Epoch 63/250\n",
            "110/110 [==============================] - 0s 92us/sample - loss: 0.0232 - acc: 0.9727\n",
            "Epoch 64/250\n",
            "110/110 [==============================] - 0s 105us/sample - loss: 0.0228 - acc: 0.9727\n",
            "Epoch 65/250\n",
            "110/110 [==============================] - 0s 103us/sample - loss: 0.0224 - acc: 0.9727\n",
            "Epoch 66/250\n",
            "110/110 [==============================] - 0s 99us/sample - loss: 0.0219 - acc: 0.9727\n",
            "Epoch 67/250\n",
            "110/110 [==============================] - 0s 116us/sample - loss: 0.0214 - acc: 0.9727\n",
            "Epoch 68/250\n",
            "110/110 [==============================] - 0s 129us/sample - loss: 0.0210 - acc: 0.9727\n",
            "Epoch 69/250\n",
            "110/110 [==============================] - 0s 105us/sample - loss: 0.0204 - acc: 0.9727\n",
            "Epoch 70/250\n",
            "110/110 [==============================] - 0s 98us/sample - loss: 0.0200 - acc: 0.9727\n",
            "Epoch 71/250\n",
            "110/110 [==============================] - 0s 108us/sample - loss: 0.0196 - acc: 0.9727\n",
            "Epoch 72/250\n",
            "110/110 [==============================] - 0s 104us/sample - loss: 0.0191 - acc: 0.9727\n",
            "Epoch 73/250\n",
            "110/110 [==============================] - 0s 112us/sample - loss: 0.0187 - acc: 0.9727\n",
            "Epoch 74/250\n",
            "110/110 [==============================] - 0s 113us/sample - loss: 0.0183 - acc: 0.9727\n",
            "Epoch 75/250\n",
            "110/110 [==============================] - 0s 106us/sample - loss: 0.0178 - acc: 0.9727\n",
            "Epoch 76/250\n",
            "110/110 [==============================] - 0s 96us/sample - loss: 0.0175 - acc: 0.9727\n",
            "Epoch 77/250\n",
            "110/110 [==============================] - 0s 108us/sample - loss: 0.0170 - acc: 0.9727\n",
            "Epoch 78/250\n",
            "110/110 [==============================] - 0s 103us/sample - loss: 0.0166 - acc: 0.9727\n",
            "Epoch 79/250\n",
            "110/110 [==============================] - 0s 93us/sample - loss: 0.0162 - acc: 0.9727\n",
            "Epoch 80/250\n",
            "110/110 [==============================] - 0s 103us/sample - loss: 0.0158 - acc: 0.9727\n",
            "Epoch 81/250\n",
            "110/110 [==============================] - 0s 99us/sample - loss: 0.0155 - acc: 0.9727\n",
            "Epoch 82/250\n",
            "110/110 [==============================] - 0s 109us/sample - loss: 0.0151 - acc: 0.9727\n",
            "Epoch 83/250\n",
            "110/110 [==============================] - 0s 102us/sample - loss: 0.0148 - acc: 0.9727\n",
            "Epoch 84/250\n",
            "110/110 [==============================] - 0s 101us/sample - loss: 0.0144 - acc: 0.9727\n",
            "Epoch 85/250\n",
            "110/110 [==============================] - 0s 101us/sample - loss: 0.0141 - acc: 0.9727\n",
            "Epoch 86/250\n",
            "110/110 [==============================] - 0s 106us/sample - loss: 0.0137 - acc: 0.9727\n",
            "Epoch 87/250\n",
            "110/110 [==============================] - 0s 120us/sample - loss: 0.0134 - acc: 0.9727\n",
            "Epoch 88/250\n",
            "110/110 [==============================] - 0s 124us/sample - loss: 0.0130 - acc: 0.9818\n",
            "Epoch 89/250\n",
            "110/110 [==============================] - 0s 109us/sample - loss: 0.0127 - acc: 0.9818\n",
            "Epoch 90/250\n",
            "110/110 [==============================] - 0s 104us/sample - loss: 0.0124 - acc: 0.9818\n",
            "Epoch 91/250\n",
            "110/110 [==============================] - 0s 106us/sample - loss: 0.0121 - acc: 0.9818\n",
            "Epoch 92/250\n",
            "110/110 [==============================] - 0s 108us/sample - loss: 0.0119 - acc: 0.9818\n",
            "Epoch 93/250\n",
            "110/110 [==============================] - 0s 97us/sample - loss: 0.0116 - acc: 0.9909\n",
            "Epoch 94/250\n",
            "110/110 [==============================] - 0s 93us/sample - loss: 0.0114 - acc: 0.9909\n",
            "Epoch 95/250\n",
            "110/110 [==============================] - 0s 80us/sample - loss: 0.0110 - acc: 0.9909\n",
            "Epoch 96/250\n",
            "110/110 [==============================] - 0s 97us/sample - loss: 0.0107 - acc: 0.9909\n",
            "Epoch 97/250\n",
            "110/110 [==============================] - 0s 130us/sample - loss: 0.0104 - acc: 0.9909\n",
            "Epoch 98/250\n",
            "110/110 [==============================] - 0s 125us/sample - loss: 0.0103 - acc: 0.9909\n",
            "Epoch 99/250\n",
            "110/110 [==============================] - 0s 118us/sample - loss: 0.0100 - acc: 0.9909\n",
            "Epoch 100/250\n",
            "110/110 [==============================] - 0s 116us/sample - loss: 0.0097 - acc: 0.9909\n",
            "Epoch 101/250\n",
            "110/110 [==============================] - 0s 120us/sample - loss: 0.0095 - acc: 0.9909\n",
            "Epoch 102/250\n",
            "110/110 [==============================] - 0s 120us/sample - loss: 0.0092 - acc: 0.9909\n",
            "Epoch 103/250\n",
            "110/110 [==============================] - 0s 112us/sample - loss: 0.0090 - acc: 0.9909\n",
            "Epoch 104/250\n",
            "110/110 [==============================] - 0s 136us/sample - loss: 0.0089 - acc: 1.0000\n",
            "Epoch 105/250\n",
            "110/110 [==============================] - 0s 117us/sample - loss: 0.0086 - acc: 1.0000\n",
            "Epoch 106/250\n",
            "110/110 [==============================] - 0s 105us/sample - loss: 0.0085 - acc: 1.0000\n",
            "Epoch 107/250\n",
            "110/110 [==============================] - 0s 122us/sample - loss: 0.0082 - acc: 1.0000\n",
            "Epoch 108/250\n",
            "110/110 [==============================] - 0s 123us/sample - loss: 0.0080 - acc: 1.0000\n",
            "Epoch 109/250\n",
            "110/110 [==============================] - 0s 125us/sample - loss: 0.0079 - acc: 1.0000\n",
            "Epoch 110/250\n",
            "110/110 [==============================] - 0s 86us/sample - loss: 0.0077 - acc: 1.0000\n",
            "Epoch 111/250\n",
            "110/110 [==============================] - 0s 121us/sample - loss: 0.0075 - acc: 1.0000\n",
            "Epoch 112/250\n",
            "110/110 [==============================] - 0s 130us/sample - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 113/250\n",
            "110/110 [==============================] - 0s 118us/sample - loss: 0.0072 - acc: 1.0000\n",
            "Epoch 114/250\n",
            "110/110 [==============================] - 0s 109us/sample - loss: 0.0070 - acc: 1.0000\n",
            "Epoch 115/250\n",
            "110/110 [==============================] - 0s 105us/sample - loss: 0.0068 - acc: 1.0000\n",
            "Epoch 116/250\n",
            "110/110 [==============================] - 0s 93us/sample - loss: 0.0067 - acc: 1.0000\n",
            "Epoch 117/250\n",
            "110/110 [==============================] - 0s 109us/sample - loss: 0.0065 - acc: 1.0000\n",
            "Epoch 118/250\n",
            "110/110 [==============================] - 0s 98us/sample - loss: 0.0064 - acc: 1.0000\n",
            "Epoch 119/250\n",
            "110/110 [==============================] - 0s 97us/sample - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 120/250\n",
            "110/110 [==============================] - 0s 84us/sample - loss: 0.0061 - acc: 1.0000\n",
            "Epoch 121/250\n",
            "110/110 [==============================] - 0s 91us/sample - loss: 0.0060 - acc: 1.0000\n",
            "Epoch 122/250\n",
            "110/110 [==============================] - 0s 94us/sample - loss: 0.0058 - acc: 1.0000\n",
            "Epoch 123/250\n",
            "110/110 [==============================] - 0s 84us/sample - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 124/250\n",
            "110/110 [==============================] - 0s 104us/sample - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 125/250\n",
            "110/110 [==============================] - 0s 95us/sample - loss: 0.0054 - acc: 1.0000\n",
            "Epoch 126/250\n",
            "110/110 [==============================] - 0s 109us/sample - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 127/250\n",
            "110/110 [==============================] - 0s 114us/sample - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 128/250\n",
            "110/110 [==============================] - 0s 90us/sample - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 129/250\n",
            "110/110 [==============================] - 0s 118us/sample - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 130/250\n",
            "110/110 [==============================] - 0s 115us/sample - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 131/250\n",
            "110/110 [==============================] - 0s 105us/sample - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 132/250\n",
            "110/110 [==============================] - 0s 85us/sample - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 133/250\n",
            "110/110 [==============================] - 0s 135us/sample - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 134/250\n",
            "110/110 [==============================] - 0s 100us/sample - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 135/250\n",
            "110/110 [==============================] - 0s 113us/sample - loss: 0.0044 - acc: 1.0000\n",
            "Epoch 136/250\n",
            "110/110 [==============================] - 0s 88us/sample - loss: 0.0043 - acc: 1.0000\n",
            "Epoch 137/250\n",
            "110/110 [==============================] - 0s 104us/sample - loss: 0.0043 - acc: 1.0000\n",
            "Epoch 138/250\n",
            "110/110 [==============================] - 0s 125us/sample - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 139/250\n",
            "110/110 [==============================] - 0s 112us/sample - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 140/250\n",
            "110/110 [==============================] - 0s 103us/sample - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 141/250\n",
            "110/110 [==============================] - 0s 119us/sample - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 142/250\n",
            "110/110 [==============================] - 0s 96us/sample - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 143/250\n",
            "110/110 [==============================] - 0s 110us/sample - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 144/250\n",
            "110/110 [==============================] - 0s 107us/sample - loss: 0.0038 - acc: 1.0000\n",
            "Epoch 145/250\n",
            "110/110 [==============================] - 0s 97us/sample - loss: 0.0038 - acc: 1.0000\n",
            "Epoch 146/250\n",
            "110/110 [==============================] - 0s 105us/sample - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 147/250\n",
            "110/110 [==============================] - 0s 87us/sample - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 148/250\n",
            "110/110 [==============================] - 0s 101us/sample - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 149/250\n",
            "110/110 [==============================] - 0s 100us/sample - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 150/250\n",
            "110/110 [==============================] - 0s 130us/sample - loss: 0.0034 - acc: 1.0000\n",
            "Epoch 151/250\n",
            "110/110 [==============================] - 0s 101us/sample - loss: 0.0034 - acc: 1.0000\n",
            "Epoch 152/250\n",
            "110/110 [==============================] - 0s 98us/sample - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 153/250\n",
            "110/110 [==============================] - 0s 88us/sample - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 154/250\n",
            "110/110 [==============================] - 0s 109us/sample - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 155/250\n",
            "110/110 [==============================] - 0s 132us/sample - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 156/250\n",
            "110/110 [==============================] - 0s 138us/sample - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 157/250\n",
            "110/110 [==============================] - 0s 104us/sample - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 158/250\n",
            "110/110 [==============================] - 0s 102us/sample - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 159/250\n",
            "110/110 [==============================] - 0s 99us/sample - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 160/250\n",
            "110/110 [==============================] - 0s 84us/sample - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 161/250\n",
            "110/110 [==============================] - 0s 104us/sample - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 162/250\n",
            "110/110 [==============================] - 0s 81us/sample - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 163/250\n",
            "110/110 [==============================] - 0s 93us/sample - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 164/250\n",
            "110/110 [==============================] - 0s 88us/sample - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 165/250\n",
            "110/110 [==============================] - 0s 86us/sample - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 166/250\n",
            "110/110 [==============================] - 0s 92us/sample - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 167/250\n",
            "110/110 [==============================] - 0s 96us/sample - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 168/250\n",
            "110/110 [==============================] - 0s 86us/sample - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 169/250\n",
            "110/110 [==============================] - 0s 84us/sample - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 170/250\n",
            "110/110 [==============================] - 0s 77us/sample - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 171/250\n",
            "110/110 [==============================] - 0s 89us/sample - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 172/250\n",
            "110/110 [==============================] - 0s 79us/sample - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 173/250\n",
            "110/110 [==============================] - 0s 86us/sample - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 174/250\n",
            "110/110 [==============================] - 0s 105us/sample - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 175/250\n",
            "110/110 [==============================] - 0s 75us/sample - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 176/250\n",
            "110/110 [==============================] - 0s 85us/sample - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 177/250\n",
            "110/110 [==============================] - 0s 86us/sample - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 178/250\n",
            "110/110 [==============================] - 0s 92us/sample - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 179/250\n",
            "110/110 [==============================] - 0s 121us/sample - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 180/250\n",
            "110/110 [==============================] - 0s 77us/sample - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 181/250\n",
            "110/110 [==============================] - 0s 99us/sample - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 182/250\n",
            "110/110 [==============================] - 0s 103us/sample - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 183/250\n",
            "110/110 [==============================] - 0s 108us/sample - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 184/250\n",
            "110/110 [==============================] - 0s 126us/sample - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 185/250\n",
            "110/110 [==============================] - 0s 101us/sample - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 186/250\n",
            "110/110 [==============================] - 0s 99us/sample - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 187/250\n",
            "110/110 [==============================] - 0s 132us/sample - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 188/250\n",
            "110/110 [==============================] - 0s 100us/sample - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 189/250\n",
            "110/110 [==============================] - 0s 98us/sample - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 190/250\n",
            "110/110 [==============================] - 0s 112us/sample - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 191/250\n",
            "110/110 [==============================] - 0s 107us/sample - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 192/250\n",
            "110/110 [==============================] - 0s 97us/sample - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 193/250\n",
            "110/110 [==============================] - 0s 89us/sample - loss: 0.0019 - acc: 1.0000\n",
            "Epoch 194/250\n",
            "110/110 [==============================] - 0s 77us/sample - loss: 0.0019 - acc: 1.0000\n",
            "Epoch 195/250\n",
            "110/110 [==============================] - 0s 79us/sample - loss: 0.0019 - acc: 1.0000\n",
            "Epoch 196/250\n",
            "110/110 [==============================] - 0s 92us/sample - loss: 0.0019 - acc: 1.0000\n",
            "Epoch 197/250\n",
            "110/110 [==============================] - 0s 82us/sample - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 198/250\n",
            "110/110 [==============================] - 0s 81us/sample - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 199/250\n",
            "110/110 [==============================] - 0s 96us/sample - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 200/250\n",
            "110/110 [==============================] - 0s 82us/sample - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 201/250\n",
            "110/110 [==============================] - 0s 113us/sample - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 202/250\n",
            "110/110 [==============================] - 0s 121us/sample - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 203/250\n",
            "110/110 [==============================] - 0s 155us/sample - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 204/250\n",
            "110/110 [==============================] - 0s 130us/sample - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 205/250\n",
            "110/110 [==============================] - 0s 91us/sample - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 206/250\n",
            "110/110 [==============================] - 0s 98us/sample - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 207/250\n",
            "110/110 [==============================] - 0s 74us/sample - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 208/250\n",
            "110/110 [==============================] - 0s 97us/sample - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 209/250\n",
            "110/110 [==============================] - 0s 111us/sample - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 210/250\n",
            "110/110 [==============================] - 0s 88us/sample - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 211/250\n",
            "110/110 [==============================] - 0s 78us/sample - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 212/250\n",
            "110/110 [==============================] - 0s 82us/sample - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 213/250\n",
            "110/110 [==============================] - 0s 99us/sample - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 214/250\n",
            "110/110 [==============================] - 0s 120us/sample - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 215/250\n",
            "110/110 [==============================] - 0s 88us/sample - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 216/250\n",
            "110/110 [==============================] - 0s 95us/sample - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 217/250\n",
            "110/110 [==============================] - 0s 85us/sample - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 218/250\n",
            "110/110 [==============================] - 0s 87us/sample - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 219/250\n",
            "110/110 [==============================] - 0s 84us/sample - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 220/250\n",
            "110/110 [==============================] - 0s 96us/sample - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 221/250\n",
            "110/110 [==============================] - 0s 83us/sample - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 222/250\n",
            "110/110 [==============================] - 0s 106us/sample - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 223/250\n",
            "110/110 [==============================] - 0s 98us/sample - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 224/250\n",
            "110/110 [==============================] - 0s 92us/sample - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 225/250\n",
            "110/110 [==============================] - 0s 90us/sample - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 226/250\n",
            "110/110 [==============================] - 0s 100us/sample - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 227/250\n",
            "110/110 [==============================] - 0s 134us/sample - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 228/250\n",
            "110/110 [==============================] - 0s 81us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 229/250\n",
            "110/110 [==============================] - 0s 86us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 230/250\n",
            "110/110 [==============================] - 0s 89us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 231/250\n",
            "110/110 [==============================] - 0s 93us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 232/250\n",
            "110/110 [==============================] - 0s 99us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 233/250\n",
            "110/110 [==============================] - 0s 112us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 234/250\n",
            "110/110 [==============================] - 0s 75us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 235/250\n",
            "110/110 [==============================] - 0s 83us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 236/250\n",
            "110/110 [==============================] - 0s 78us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 237/250\n",
            "110/110 [==============================] - 0s 89us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 238/250\n",
            "110/110 [==============================] - 0s 77us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 239/250\n",
            "110/110 [==============================] - 0s 130us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 240/250\n",
            "110/110 [==============================] - 0s 111us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 241/250\n",
            "110/110 [==============================] - 0s 97us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 242/250\n",
            "110/110 [==============================] - 0s 90us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 243/250\n",
            "110/110 [==============================] - 0s 85us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 244/250\n",
            "110/110 [==============================] - 0s 77us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 245/250\n",
            "110/110 [==============================] - 0s 80us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 246/250\n",
            "110/110 [==============================] - 0s 68us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 247/250\n",
            "110/110 [==============================] - 0s 118us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 248/250\n",
            "110/110 [==============================] - 0s 76us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 249/250\n",
            "110/110 [==============================] - 0s 78us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 250/250\n",
            "110/110 [==============================] - 0s 81us/sample - loss: 0.0010 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7936bd2dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj-Pj96PRryw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "60f38806-6d82-4b27-bcfe-717dbb1298aa"
      },
      "source": [
        "ann.summary()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  330       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  140       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  126       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  7         \n",
            "=================================================================\n",
            "Total params: 603\n",
            "Trainable params: 603\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RChH_y0SezP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "19d591ce-d8eb-4383-b189-4fbd6791e764"
      },
      "source": [
        "loss_df = pd.DataFrame(ann.history.history)\n",
        "loss_df.plot()\n",
        "#plot the loss history"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7936b81f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIlIh/j/nF1gm/D4SqAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTmls3B1Th7O",
        "colab_type": "text"
      },
      "source": [
        "### We have built the following models\n",
        "**Logistic Regression Classifier**<br/>\n",
        "**K Nearest Neigbours Classifier**<br/>\n",
        "**Random Forest Classifier**<br/>\n",
        "**Naive Bayes Classifier**<br/>\n",
        "**Artificial Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eJZNYO7TmQk",
        "colab_type": "text"
      },
      "source": [
        "We find that the accuracy is almost same for this dataset but it usually changes.<br/>\n",
        "**So we finaly built a model which can predict with an accuracy of about 100%**.<br/>\n",
        "Apart from the conclusion of the data, I would like to add that the usage of ANNs, CNNs and RNNs are not always neccesary. Sometimes the Logistic Regression itself can prove how powerful it is.<br/>\n",
        "But... If you are really into getting the best accuracy, Neural Networks are the best way as far my knowledge goes.\n",
        "\n",
        "**Let's pray that everyone shall live happily and there should be no divorces**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W16_jy-RTxI0",
        "colab_type": "text"
      },
      "source": [
        "## Thank You\n",
        "**BONDA SAIM**<br/>\n",
        "**Student @National Institute of Technology**<br/>\n",
        "**9441369824**<br/>\n",
        "**saim.sp.sonu@gmail.com**"
      ]
    }
  ]
}